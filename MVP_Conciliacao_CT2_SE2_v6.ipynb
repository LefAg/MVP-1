{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LefAg/MVP-1/blob/main/MVP_Conciliacao_CT2_SE2_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e44dbcd",
      "metadata": {
        "id": "2e44dbcd"
      },
      "source": [
        "# MVP Disciplina: Sprint: Machine Learning & Analytics\n",
        "\n",
        "Aluno: Leandro F\n",
        "\n",
        "Análise Exploratória Contábil Conciliador — Contábil (CT2) × Fianceiro (SE2)\n",
        "(Notebook com **itens numerados** e **descrições**.\n",
        "\n",
        "O MVP automatiza a conciliação contábil entre lançamentos do CT2 e movimentos do SE2.\n",
        "Ele lê os arquivos (via GitHub RAW/Colab), normaliza contas, cria visão Débito/Crédito, e aplica regras R1–R3 (NF|Código, Código|Valor, NF|Valor) para parear D×C por conta. O que não fecha vira “aberto”; nesses, o sistema busca evidências no SE2 e marca SE2_Presente.\n",
        "Gera relatórios em Excel e PDF (ex.: Saldo_CT2_abertos, CT2_pairs_matches com resumo por conta, Pendências por competência) e tabelas enriquecidas com chaves, NF, Código da Parte e Fornecedor.\n",
        "Por fim, um módulo de ML (RandomForest + IsolationForest) prioriza abertos com maior chance de estarem no SE2 e destaca anomalias — entregando um ML_predictions.xlsx com “Top Suspeitos” e métricas.\n",
        "\n",
        "**Fluxo do MVP (visão de arquitetura)**\n",
        "\n",
        "Entrada & Setup: leitura de CT2.xlsx e SE2.xlsx (GitHub RAW ou local); parâmetros (contas foco, tolerância em centavos, competência) e saída em /content/OUT.\n",
        "\n",
        "Normalização & Enriquecimento: padroniza contas, extrai NF e Código da Parte do histórico; cria chaves combinadas e valores em centavos.\n",
        "\n",
        "Match intra-CT2 (R1–R3): pareia D×C por Conta; gera pares e restantes (abertos).\n",
        "\n",
        "Confirmação CT2 × SE2: nos abertos, sinaliza SE2_Presente por chaves (com tolerância).\n",
        "\n",
        "Relatórios principais:\n",
        "\n",
        "Saldo_CT2_abertos (Data, conta, Lote, Hist, NF, CodParte, VlrDC, SE2) + total por conta;\n",
        "\n",
        "CT2_pairs_matches (Data, conta, NF, CodParte, valor débito/valor crédito/débito–crédito = 0) + resumo por conta;\n",
        "\n",
        "Pendências Finais por competência.\n",
        "\n",
        "Enriquecidos & Conferência:\n",
        "\n",
        "CT2/SE2 enriquecidos (NF, Código, Fornecedor e chaves);\n",
        "\n",
        "Planilhas de apoio (Visao_DC, Resumo_Conta, CT2_Matches_tol, Conferencia_CT2_vs_SE2).\n",
        "\n",
        "ML (supervisionado + anomalias):\n",
        "\n",
        "RandomForest prevê SE2_Presente; IsolationForest aponta anomalias nos ausentes;\n",
        "\n",
        "Comparação com baseline (Dummy/LogReg) via CV;\n",
        "\n",
        "Saídas: ML_predictions.xlsx (Comparação_CV, Predições, Top_Suspeitos, Matriz de Confusão, Métricas) + modelos .pkl.\n",
        "\n",
        "**Benefícios**\n",
        "\n",
        "Velocidade: conciliação e relatórios em minutos.\n",
        "\n",
        "Rastreabilidade: regras claras (R1–R3), PDFs/Excels versionáveis.\n",
        "\n",
        "Foco da equipe: priorização de pendências por risco/probabilidade (ML).\n",
        "\n",
        "Portabilidade: roda no Colab (sem instalação local) e lê dados do GitHub.\n",
        "\n",
        "**Como executar (resumo)**\n",
        "\n",
        "Abrir o notebook no Google Colab.\n",
        "\n",
        "Item 1 – Parâmetros: URLs RAW do CT2.xlsx e SE2.xlsx e outdir=/content/OUT.\n",
        "\n",
        "Executar tudo; no final, baixar o ZIP dos relatórios.\n",
        "\n",
        "**Próximos passos**\n",
        "\n",
        "Enriquecer features (datas de vencimento, natureza do histórico, cluster por parceiro).\n",
        "\n",
        "Tuning de hiperparâmetros e explicabilidade (SHAP).\n",
        "\n",
        "Dashboard leve (Streamlit) consumindo os Excels/CSVs gerados.\n",
        "\n",
        "Integração com ERP/filas para rodar diariamente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34634b17",
      "metadata": {
        "id": "34634b17"
      },
      "source": [
        "## 1) Parâmetros de execução\n",
        "Entradas padrão (parametrizáveis) e diretório de saída.\n",
        "O que faz: define as entradas e opções do pipeline.\n",
        "\n",
        "Principais parâmetros:\n",
        "\n",
        "ct2_path, se2_path: caminhos dos arquivos de entrada (CT2.xlsx, SE2.xlsx).\n",
        "\n",
        "outdir: pasta de saída (C:\\MVP\\MVP1\\OUT por padrão).\n",
        "\n",
        "conta_clientes, conta_fornecedores: contas foco para o relatório de abertos.\n",
        "\n",
        "tolerancia_centavos: tolerância para “bater” valores (ex.: 1 = ± R$0,01).\n",
        "\n",
        "competencia: rótulo de competência (ex.: 2025-09).\n",
        "\n",
        "Saída: cria a pasta OUT se não existir e imprime um resumo dos parâmetros usados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a2548bd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2548bd6",
        "outputId": "725d4af8-264b-46df-c727-f592499b79e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: baixado de https://raw.githubusercontent.com/LefAg/MVP-1/main/CT2.xlsx -> /content/data/CT2.xlsx\n",
            "OK: baixado de https://raw.githubusercontent.com/LefAg/MVP-1/main/SE2.xlsx -> /content/data/SE2.xlsx\n",
            "CT2: /content/data/CT2.xlsx\n",
            "SE2: /content/data/SE2.xlsx\n",
            "OUT: /content/OUT\n",
            "Conta clientes: 11030101001 | Conta fornecedores: 21010201001 | Tolerância (centavos): 1 | Competência: 2025-09\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Parâmetros de execução — híbrido (Colab + VS Code) sem 'requests' (usa urllib)\n",
        "\n",
        "import os, ssl, urllib.request\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Detecta Colab vs local\n",
        "IS_COLAB = Path(\"/content\").exists()\n",
        "\n",
        "# ===== URLs RAW do GitHub (ajuste o CT2 se necessário) =====\n",
        "URL_CT2 = os.environ.get(\"MVP_CT2_URL\", \"https://raw.githubusercontent.com/LefAg/MVP-1/main/CT2.xlsx\")\n",
        "URL_SE2 = os.environ.get(\"MVP_SE2_URL\", \"https://raw.githubusercontent.com/LefAg/MVP-1/main/SE2.xlsx\")\n",
        "\n",
        "# ===== Fallback local (opcional em VS Code) =====\n",
        "ct2_local = os.environ.get(\"MVP_CT2_PATH\", r\"C:\\MVP\\MVP1\\CT2.xlsx\")\n",
        "se2_local = os.environ.get(\"MVP_SE2_PATH\", r\"C:\\MVP\\MVP1\\SE2.xlsx\")\n",
        "\n",
        "# ===== Saída =====\n",
        "outdir = os.environ.get(\"MVP_OUTDIR\", \"/content/OUT\" if IS_COLAB else r\"C:\\MVP\\MVP1\\OUT\")\n",
        "Path(outdir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ===== Pasta de dados =====\n",
        "datadir = Path(\"/content/data\" if IS_COLAB else \"./data\")\n",
        "datadir.mkdir(parents=True, exist_ok=True)\n",
        "ct2_path = datadir / \"CT2.xlsx\"\n",
        "se2_path = datadir / \"SE2.xlsx\"\n",
        "\n",
        "# Contexto SSL (evita erros em ambientes corporativos)\n",
        "_SSL_CTX = ssl.create_default_context()\n",
        "\n",
        "def _download_xlsx(url: str, dest_path: Path):\n",
        "    with urllib.request.urlopen(url, context=_SSL_CTX) as resp:\n",
        "        dest_path.write_bytes(resp.read())\n",
        "\n",
        "def _ensure_file(path_target: Path, url_raw: str, local_fallback: str) -> str:\n",
        "    # tenta URL RAW\n",
        "    if url_raw and url_raw.startswith(\"http\"):\n",
        "        try:\n",
        "            _download_xlsx(url_raw, path_target)\n",
        "            print(f\"OK: baixado de {url_raw} -> {path_target}\")\n",
        "            return str(path_target)\n",
        "        except Exception as e:\n",
        "            print(f\"[AVISO] Falha ao baixar {url_raw}: {e}\")\n",
        "    # fallback local\n",
        "    if local_fallback and Path(local_fallback).exists():\n",
        "        print(f\"Usando arquivo local: {local_fallback}\")\n",
        "        return local_fallback\n",
        "    raise FileNotFoundError(f\"Não foi possível obter o arquivo. URL={url_raw} | local={local_fallback}\")\n",
        "\n",
        "ct2_path = _ensure_file(ct2_path, URL_CT2, ct2_local)\n",
        "se2_path = _ensure_file(se2_path, URL_SE2, se2_local)\n",
        "\n",
        "# Parâmetros globais usados no restante do notebook\n",
        "conta_clientes      = os.environ.get(\"MVP_CONTA_CLIENTES\", \"11030101001\")\n",
        "conta_fornecedores  = os.environ.get(\"MVP_CONTA_FORNEC\",  \"21010201001\")\n",
        "tolerancia_centavos = int(os.environ.get(\"MVP_TOL_CENT\", \"1\"))\n",
        "competencia         = os.environ.get(\"MVP_COMPETENCIA\", datetime.now().strftime(\"%Y-%m\"))\n",
        "\n",
        "print(\"CT2:\", ct2_path)\n",
        "print(\"SE2:\", se2_path)\n",
        "print(\"OUT:\", outdir)\n",
        "print(\"Conta clientes:\", conta_clientes, \"| Conta fornecedores:\", conta_fornecedores,\n",
        "      \"| Tolerância (centavos):\", tolerancia_centavos, \"| Competência:\", competencia)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551211bd",
      "metadata": {
        "id": "551211bd"
      },
      "source": [
        "## 2) Imports, normalizações e utilitários\n",
        "O que faz: carrega bibliotecas e define funções auxiliares.\n",
        "\n",
        "Funções-chave:\n",
        "\n",
        "normalize_conta(x): limpa e padroniza o número da conta (remove pontos, traços, etc.).\n",
        "\n",
        "extract_nf_and_codigo(hist): tenta extrair NF (ex.: “NF 12345”) e Código de Parte (heurística: 8 dígitos = PJ, 9 dígitos = PF) a partir do histórico.\n",
        "\n",
        "to_centavos_abs(v): converte o valor monetário para centavos absolutos (int), preservando sinal separadamente.\n",
        "\n",
        "write_pdf_table(path, title, df): gera um PDF simples com cabeçalho e até ~28 linhas do DataFrame.\n",
        "\n",
        "as_str(series): força dtype string (pandas) e preenche NA com \"\" (evita erros ao concatenar).\n",
        "\n",
        "centavos_to_str(series): transforma a coluna com centavos (int/float/NA) em string segura.\n",
        "\n",
        "Resolver robusto de colunas (CT2/SE2): mapeia nomes “parecidos” (ignorando acentos/variações) para achar as colunas de Data, Histórico, Débito, Crédito, Valor, Lote, NF, Código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "071ad1f6",
      "metadata": {
        "id": "071ad1f6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2) Utilitários (normalização, extrações, PDF compacto)\n",
        "\n",
        "# garante reportlab no Colab se ainda não estiver instalado\n",
        "import sys, subprocess\n",
        "try:\n",
        "    from reportlab.pdfgen import canvas\n",
        "    from reportlab.lib.pagesizes import landscape, A4\n",
        "    from reportlab.lib.units import cm\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"reportlab\"])\n",
        "    from reportlab.pdfgen import canvas\n",
        "    from reportlab.lib.pagesizes import landscape, A4\n",
        "    from reportlab.lib.units import cm\n",
        "\n",
        "import re, unicodedata\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import re, unicodedata\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import landscape, A4\n",
        "from reportlab.lib.units import cm\n",
        "def normalize_conta(x):\n",
        "    if pd.isna(x): return None\n",
        "    s = str(x).strip()\n",
        "    if s.endswith(\".0\"): s = s[:-2]\n",
        "    return re.sub(r\"\\D\", \"\", s)\n",
        "def extract_nf_and_codigo(hist):\n",
        "    if pd.isna(hist): return (None, None, None)\n",
        "    s = str(hist)\n",
        "    cod_match = re.search(r\"\\b(\\d{8,9})\\b\", s)\n",
        "    codigo = cod_match.group(1) if cod_match else None\n",
        "    tipo = \"PJ_8d\" if (codigo and len(codigo)==8) else (\"PF_9d\" if (codigo and len(codigo)==9) else \"OUTRO\")\n",
        "    nf_match = re.search(r\"(?:N\\.?F\\.?\\s*[\\-:]?\\s*)(\\d+)\", s, flags=re.IGNORECASE)\n",
        "    if nf_match: nf = nf_match.group(1)\n",
        "    else:\n",
        "        nf_fallback = re.search(r\"\\b(\\d{4,})\\b\", s)\n",
        "        nf = nf_fallback.group(1) if nf_fallback else None\n",
        "    return (nf, codigo, tipo)\n",
        "def to_centavos_abs(v):\n",
        "    if pd.isna(v): return None\n",
        "    return int(round(abs(float(v))*100))\n",
        "def as_str(series): return series.astype(\"string\").fillna(\"\")\n",
        "def centavos_to_str(series): return series.apply(lambda x: \"\" if pd.isna(x) else str(int(x)))\n",
        "DISPLAY_NAMES = {\"idx_src\":\"idx\",\"ValorOrig\":\"VlrOrig\",\"ValorDC\":\"VlrDC\",\"ValorAbsCent\":\"ValCent\",\"CodigoParte\":\"CodParte\",\n",
        "                 \"K_NF|CodigoParte\":\"Key NF|Cod\",\"K_NF|Valor\":\"Key NF|Val\",\"K_CodigoParte|Valor\":\"Key Cod|Val\",\n",
        "                 \"K_NF|CodigoParte|Valor\":\"Key NF|Cod|Val\",\"SE2_Presente\":\"SE2?\",\"Conta_Foco\":\"ContaFoco\",\"competencia\":\"Compet\"}\n",
        "def _wrap_text(s: str, max_chars: int, max_lines: int = 2):\n",
        "    tokens = re.split(r'([ _\\|])', s); lines, cur = [], \"\"\n",
        "    for t in tokens:\n",
        "        if len(cur)+len(t) <= max_chars: cur += t\n",
        "        else:\n",
        "            lines.append(cur.strip()); cur = t.strip()\n",
        "            if len(lines) >= max_lines-1: break\n",
        "    if cur: lines.append(cur.strip())\n",
        "    if len(lines) > max_lines: lines = lines[:max_lines]\n",
        "    if len(\"\".join(tokens)) > sum(len(l) for l in lines):\n",
        "        lines[-1] = (lines[-1][:max(0, max_chars-1)] + \"…\") if len(lines[-1]) >= max_chars else (lines[-1] + \"…\")\n",
        "    return lines[:max_lines]\n",
        "def write_pdf_table_wrapped(pdf_path, title, df, keep_columns=None, drop_hist=True, font_size=7, max_rows_pdf=None):\n",
        "    df = df.copy()\n",
        "    if drop_hist and \"Hist\" in df.columns: df = df.drop(columns=[\"Hist\"])\n",
        "    if keep_columns: df = df[[c for c in keep_columns if c in df.columns]]\n",
        "    if max_rows_pdf is not None: df = df.head(int(max_rows_pdf))\n",
        "    df = df.rename(columns={c: DISPLAY_NAMES.get(c,c) for c in df.columns})\n",
        "    c = canvas.Canvas(pdf_path, pagesize=landscape(A4))\n",
        "    width, height = landscape(A4); margin = 1.1*cm; y = height - margin\n",
        "    c.setFont(\"Helvetica-Bold\", 13); c.drawString(margin, y, str(title)); y -= 0.6*cm\n",
        "    cols = list(df.columns); ncols = max(1,len(cols)); col_w = (width-2*margin)/ncols\n",
        "    c.setFont(\"Helvetica-Bold\", font_size); max_chars = max(8, int(col_w/4.0)); line_h = 0.32*cm; header_h = line_h*2\n",
        "    for i, col in enumerate(cols):\n",
        "        for j, ln in enumerate(_wrap_text(str(col), max_chars, 2)):\n",
        "            c.drawString(margin + i*col_w, y - j*line_h, ln)\n",
        "    y -= header_h + 0.1*cm\n",
        "    c.setFont(\"Helvetica\", font_size); approx = max(8, int(col_w/4.2)); row_h = 0.36*cm\n",
        "    max_rows_page = int((y - margin)/ row_h)\n",
        "    idx = 0; total = len(df)\n",
        "    while idx < total:\n",
        "        rpage = min(max_rows_page, total-idx)\n",
        "        for r in range(rpage):\n",
        "            row = df.iloc[idx+r]; yy = y - r*row_h\n",
        "            for i,col in enumerate(cols):\n",
        "                txt = \"\" if pd.isna(row[col]) else str(row[col])\n",
        "                if len(txt) > approx: txt = txt[:approx-1]+\"…\"\n",
        "                c.drawString(margin + i*col_w, yy, txt)\n",
        "        idx += rpage\n",
        "        if idx < total:\n",
        "            c.showPage(); y = height - margin\n",
        "            c.setFont(\"Helvetica-Bold\", 13); c.drawString(margin, y, str(title)); y -= 0.6*cm\n",
        "            c.setFont(\"Helvetica-Bold\", font_size)\n",
        "            for i, col in enumerate(cols):\n",
        "                for j, ln in enumerate(_wrap_text(str(col), max_chars, 2)):\n",
        "                    c.drawString(margin + i*col_w, y - j*line_h, ln)\n",
        "            y -= header_h + 0.1*cm; c.setFont(\"Helvetica\", font_size)\n",
        "    c.showPage(); c.save()\n",
        "def _norm(s): s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\",\"ignore\").decode(\"ascii\"); s = re.sub(r\"[^a-z0-9]+\",\" \", s.lower()).strip(); return re.sub(r\"\\s+\",\" \", s)\n",
        "def _build_norm_map(df): return { _norm(c): c for c in df.columns }\n",
        "def _pick(n2o, cands, contains_any=None):\n",
        "    for c in cands:\n",
        "        if c in n2o: return n2o[c]\n",
        "    if contains_any:\n",
        "        for n,orig in n2o.items():\n",
        "            if any(tok in n for tok in contains_any): return orig\n",
        "    return None\n",
        "def resolver_ct2_cols(df):\n",
        "    n = _build_norm_map(df)\n",
        "    return {\"data\": _pick(n, [\"data lcto\",\"data lanc\",\"dt lanc\",\"data\",\"dtlanc\",\"dt\"], [\"data\",\"dt\"]),\n",
        "            \"hist\": _pick(n, [\"hist lanc\",\"historico lanc\",\"historico\",\"hist\",\"descricao historico\"], [\"hist\",\"descr\"]),\n",
        "            \"cta_debito\": _pick(n, [\"cta debito\",\"conta debito\",\"conta deb\",\"debito conta\",\"conta d\",\"cta d\",\"debito\"], [\"deb\"]),\n",
        "            \"cta_credito\": _pick(n, [\"cta credito\",\"conta credito\",\"conta cred\",\"credito conta\",\"conta c\",\"cta c\",\"credito\"], [\"cred\"]),\n",
        "            \"valor\": _pick(n, [\"valor lanc\",\"vlr lanc\",\"valor lancamento\",\"valor\",\"vlr\",\"vlr lancamento\"], [\"valor\",\"vlr\"]),\n",
        "            \"lote\": _pick(n, [\"lote\",\"num lote\",\"numero lote\",\"nr lote\"], [\"lote\"])}\n",
        "def resolver_se2_cols(df):\n",
        "    n = _build_norm_map(df)\n",
        "    return {\"data\": _pick(n, [\"data mov\",\"data\",\"dt\"], [\"data\",\"dt\"]),\n",
        "            \"hist\": _pick(n, [\"historico\",\"hist\"], [\"hist\",\"descr\"]),\n",
        "            \"valor\": _pick(n, [\"valor\",\"vlr\",\"valor mov\",\"vlr mov\"], [\"valor\",\"vlr\"]),\n",
        "            \"nf\": _pick(n, [\"nf\",\"nota fiscal\",\"nr nf\",\"numero nf\"], None),\n",
        "            \"codigo\": _pick(n, [\"codigo\",\"cod parceiro\",\"codigo parte\",\"cliente fornecedor\",\"cod cli for\",\"cod parte\",\"codigo cliente\",\"codigo fornecedor\"], [\"cod\",\"cliente\",\"fornecedor\",\"parte\"])}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f27f6f",
      "metadata": {
        "id": "b4f27f6f"
      },
      "source": [
        "## 3) Carga e mapeamento\n",
        "O que faz: lê CT2.xlsx e SE2.xlsx e usa o resolver para encontrar as colunas relevantes.\n",
        "\n",
        "Validações: se não encontrar pelo menos Valor e Histórico em CT2, para com erro orientando a ajustar o resolver (porque sem esses campos não dá para seguir).\n",
        "\n",
        "Saída: imprime shape de cada base e o mapeamento detectado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e862b85d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e862b85d",
        "outputId": "da69029b-3180-4bb6-cff7-0ab79ba25c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CT2: (1596, 16) | SE2: (356, 8)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ct2 = pd.read_excel(ct2_path); se2 = pd.read_excel(se2_path)\n",
        "cols_ct2 = resolver_ct2_cols(ct2); cols_se2 = resolver_se2_cols(se2)\n",
        "assert cols_ct2[\"valor\"] and cols_ct2[\"hist\"], f\"CT2 precisa de Valor/Hist. Mapeamento: {cols_ct2}\"\n",
        "print(\"CT2:\", ct2.shape, \"| SE2:\", se2.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1e72e4",
      "metadata": {
        "id": "3b1e72e4"
      },
      "source": [
        "## 4) Transformação D/C (sem duplicações artificiais)\n",
        "O que faz: cria a visão “movimento” com uma linha D (valor positivo) e uma linha C (valor negativo) por lançamento, usando as contas de débito e crédito da CT2.\n",
        "\n",
        "Campos gerados:\n",
        "Conta, DC (“D” ou “C”), ValorDC (+ para D; − para C), além de Data, Lote, Hist e ValorOrig.\n",
        "\n",
        "Validação de integridade: checa se a soma da visão D/C bate com a soma da CT2 original (garante que não criamos/remoções indevidas de valor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "be1193d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be1193d9",
        "outputId": "874e94f5-15b9-4334-a97c-fdd880894341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linhas DC: 1809\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ct2[\"ContaDeb\"] = ct2[cols_ct2[\"cta_debito\"]].apply(normalize_conta) if cols_ct2[\"cta_debito\"] else None\n",
        "ct2[\"ContaCre\"] = ct2[cols_ct2[\"cta_credito\"]].apply(normalize_conta) if cols_ct2[\"cta_credito\"] else None\n",
        "ct2[\"ValorOrig\"] = pd.to_numeric(ct2[cols_ct2[\"valor\"]], errors=\"coerce\")\n",
        "ct2[\"HistOrig\"]  = as_str(ct2[cols_ct2[\"hist\"]])\n",
        "rows=[]\n",
        "for idx,r in ct2.iterrows():\n",
        "    base={\"idx_src\":idx,\"Data\": r.get(cols_ct2[\"data\"]) if cols_ct2[\"data\"] else r.get(ct2.columns[0]),\n",
        "          \"Lote\": r.get(\"Lote\", r.get(cols_ct2[\"lote\"])) if cols_ct2[\"lote\"] else r.get(\"Lote\"),\n",
        "          \"Hist\": r.get(\"HistOrig\"),\"ValorOrig\": r.get(\"ValorOrig\")}\n",
        "    if cols_ct2[\"cta_debito\"] and pd.notna(r[\"ContaDeb\"]):\n",
        "        rr=base.copy(); rr[\"Conta\"]=r[\"ContaDeb\"]; rr[\"DC\"]=\"D\"; rr[\"ValorDC\"]=abs(r[\"ValorOrig\"]); rows.append(rr)\n",
        "    if cols_ct2[\"cta_credito\"] and pd.notna(r[\"ContaCre\"]):\n",
        "        rr=base.copy(); rr[\"Conta\"]=r[\"ContaCre\"]; rr[\"DC\"]=\"C\"; rr[\"ValorDC\"]=-abs(r[\"ValorOrig\"]); rows.append(rr)\n",
        "visao_dc=pd.DataFrame(rows)\n",
        "print(\"Linhas DC:\", len(visao_dc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "020edfe1",
      "metadata": {
        "id": "020edfe1"
      },
      "source": [
        "## 5) Normalizações/Enriquecimento (CT2) + Chaves\n",
        "O que faz:\n",
        "\n",
        "Extrai NF, Código da Parte (e o tipo: PJ_8d, PF_9d, OUTRO) a partir do histórico da CT2.\n",
        "\n",
        "Cria o ValorAbsCent (valor absoluto em centavos, inteiro) para as chaves.\n",
        "\n",
        "Constrói as chaves de conciliação:\n",
        "\n",
        "K_NF|CodigoParte = NF + \"|\" + Codigo\n",
        "\n",
        "K_NF|Valor = NF + \"|\" + ValorAbsCent\n",
        "\n",
        "K_CodigoParte|Valor = Codigo + \"|\" + ValorAbsCent\n",
        "\n",
        "K_NF|CodigoParte|Valor = NF + \"|\" + Codigo + \"|\" + ValorAbsCent\n",
        "\n",
        "Resumo auxiliar: gera resumo_conta (Débitos, Créditos, Saldo, Qtd) por Conta × DC para conferência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0bd33344",
      "metadata": {
        "id": "0bd33344"
      },
      "outputs": [],
      "source": [
        "\n",
        "nf,codigo,tipo=[],[],[]\n",
        "for s in visao_dc[\"Hist\"].fillna(\"\"):\n",
        "    n,c,t=extract_nf_and_codigo(s); nf.append(n); codigo.append(c); tipo.append(t)\n",
        "visao_dc[\"NF\"]=as_str(pd.Series(nf))\n",
        "visao_dc[\"CodigoParte\"]=as_str(pd.Series(codigo))\n",
        "visao_dc[\"TipoParte\"]=as_str(pd.Series(tipo))\n",
        "visao_dc[\"ValorAbsCent\"]=visao_dc[\"ValorDC\"].apply(to_centavos_abs)\n",
        "val_ct2_str=centavos_to_str(visao_dc[\"ValorAbsCent\"])\n",
        "nf_ct2_str=as_str(visao_dc[\"NF\"]); cod_ct2_str=as_str(visao_dc[\"CodigoParte\"])\n",
        "visao_dc[\"K_NF|CodigoParte\"]=nf_ct2_str+\"|\"+cod_ct2_str\n",
        "visao_dc[\"K_NF|Valor\"]=nf_ct2_str+\"|\"+val_ct2_str\n",
        "visao_dc[\"K_CodigoParte|Valor\"]=cod_ct2_str+\"|\"+val_ct2_str\n",
        "visao_dc[\"K_NF|CodigoParte|Valor\"]=nf_ct2_str+\"|\"+cod_ct2_str+\"|\"+val_ct2_str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41846491",
      "metadata": {
        "id": "41846491"
      },
      "source": [
        "## 6) Transformação SE2 + chaves (valor absoluto)\n",
        "O que faz:\n",
        "\n",
        "Padroniza o histórico (Hist2) e tenta obter NF e Código:\n",
        "\n",
        "Se a SE2 já tiver colunas de NF/Código mapeadas, usa-as.\n",
        "\n",
        "Caso contrário, extrai do Hist2 com a mesma heurística da CT2.\n",
        "\n",
        "Converte Valor para número, cria ValorAbsCent.\n",
        "\n",
        "Gera as mesmas chaves usadas na CT2 (compatibilidade perfeita para comparar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2ed70ce5",
      "metadata": {
        "id": "2ed70ce5"
      },
      "outputs": [],
      "source": [
        "\n",
        "se2=se2.copy()\n",
        "se2[\"Hist2\"]=as_str(se2[cols_se2[\"hist\"]]) if cols_se2[\"hist\"] else as_str(se2.iloc[:,0])\n",
        "se2[\"NF\"]=as_str(se2[cols_se2[\"nf\"]]) if cols_se2[\"nf\"] else as_str(pd.Series([extract_nf_and_codigo(s)[0] for s in se2[\"Hist2\"].fillna(\"\")]))\n",
        "se2[\"Codigo\"]=as_str(se2[cols_se2[\"codigo\"]]) if cols_se2[\"codigo\"] else as_str(pd.Series([extract_nf_and_codigo(s)[1] for s in se2[\"Hist2\"].fillna(\"\")]))\n",
        "se2[\"Valor\"]=pd.to_numeric(se2[cols_se2[\"valor\"]], errors=\"coerce\") if cols_se2[\"valor\"] else pd.to_numeric(se2.iloc[:,1], errors=\"coerce\")\n",
        "se2[\"ValorAbsCent\"]=se2[\"Valor\"].apply(to_centavos_abs)\n",
        "nf_str, cod_str, val_str = as_str(se2[\"NF\"]), as_str(se2[\"Codigo\"]), centavos_to_str(se2[\"ValorAbsCent\"])\n",
        "se2[\"K_NF|CodigoParte\"]=nf_str+\"|\"+cod_str\n",
        "se2[\"K_NF|Valor\"]=nf_str+\"|\"+val_str\n",
        "se2[\"K_CodigoParte|Valor\"]=cod_str+\"|\"+val_str\n",
        "se2[\"K_NF|CodigoParte|Valor\"]=nf_str+\"|\"+cod_str+\"|\"+val_str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "876db7bd",
      "metadata": {
        "id": "876db7bd"
      },
      "source": [
        "## 6.1) CT2 & SE2 — enriquecidos (export)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a75e9a1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a75e9a1b",
        "outputId": "b42b9f3e-1f30-4c4b-f16a-60f38ba3b3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CT2 & SE2 enriquecidos exportados (datas normalizadas).\n"
          ]
        }
      ],
      "source": [
        "# 6.1) CT2 & SE2 — enriquecidos (export)  [VERSÃO CORRIGIDA]\n",
        "# Gera CT2/SE2 com chaves + NF + CodigoParte + Fornecedor, normalizando datas para ordenar.\n",
        "\n",
        "# garante xlsxwriter no Colab/ambiente atual\n",
        "import sys, subprocess\n",
        "try:\n",
        "    import xlsxwriter  # noqa: F401\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xlsxwriter\", \"openpyxl\"])\n",
        "    import xlsxwriter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def _to_dt(series):\n",
        "    \"\"\"Converte para datetime (NaT quando não converter).\"\"\"\n",
        "    return pd.to_datetime(series, errors=\"coerce\")\n",
        "\n",
        "# ------- CT2 enriquecido -------\n",
        "ct2_enriq = visao_dc.copy()\n",
        "\n",
        "# Fornecedor no CT2: marca o código apenas quando a conta é a de fornecedores\n",
        "ct2_enriq[\"Fornecedor\"] = np.where(\n",
        "    ct2_enriq[\"Conta\"] == conta_fornecedores,\n",
        "    ct2_enriq[\"CodigoParte\"].fillna(\"\"),\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "# Normaliza Data para ordenar sem conflito de tipos\n",
        "ct2_enriq[\"DataSort\"] = _to_dt(ct2_enriq[\"Data\"])\n",
        "\n",
        "cols_ct2_enriq = [\n",
        "    \"Data\",\"Conta\",\"DC\",\"ValorDC\",\n",
        "    \"NF\",\"CodigoParte\",\"Fornecedor\",\n",
        "    \"K_NF|CodigoParte\",\"K_CodigoParte|Valor\",\"K_NF|Valor\",\"K_NF|CodigoParte|Valor\"\n",
        "]\n",
        "cols_ct2_enriq = [c for c in cols_ct2_enriq if c in ct2_enriq.columns]\n",
        "\n",
        "ct2_enriq = (\n",
        "    ct2_enriq\n",
        "      .sort_values([\"Conta\",\"DataSort\"], kind=\"stable\")\n",
        "      .drop(columns=[\"DataSort\"])\n",
        "      [cols_ct2_enriq]\n",
        ")\n",
        "\n",
        "# ------- SE2 enriquecido -------\n",
        "# Pega a coluna de data mapeada (se existir) e normaliza\n",
        "se2_data_col = cols_se2.get(\"data\")\n",
        "se2_data_raw = se2[se2_data_col] if se2_data_col else pd.Series([pd.NaT]*len(se2), name=\"Data\")\n",
        "se2_data_sort = _to_dt(se2_data_raw)\n",
        "\n",
        "se2_enriq = pd.DataFrame({\n",
        "    \"Data\": se2_data_raw,\n",
        "    \"NF\": se2[\"NF\"],\n",
        "    \"CodigoParte\": se2[\"Codigo\"],\n",
        "    \"Fornecedor\": se2[\"Codigo\"],        # sem nome, usamos o identificador do parceiro\n",
        "    \"Valor\": se2[\"Valor\"],\n",
        "    \"ValorAbsCent\": se2[\"ValorAbsCent\"],\n",
        "    \"K_NF|CodigoParte\": se2[\"K_NF|CodigoParte\"],\n",
        "    \"K_CodigoParte|Valor\": se2[\"K_CodigoParte|Valor\"],\n",
        "    \"K_NF|Valor\": se2[\"K_NF|Valor\"],\n",
        "    \"K_NF|CodigoParte|Valor\": se2[\"K_NF|CodigoParte|Valor\"],\n",
        "})\n",
        "se2_enriq[\"DataSort\"] = se2_data_sort\n",
        "\n",
        "se2_enriq = (\n",
        "    se2_enriq\n",
        "      .sort_values([\"CodigoParte\",\"DataSort\"], kind=\"stable\")\n",
        "      .drop(columns=[\"DataSort\"])\n",
        ")\n",
        "\n",
        "# ------- Exporta Excel (duas abas) -------\n",
        "out_base = Path(outdir)\n",
        "with pd.ExcelWriter(out_base / \"CT2_SE2_enriquecidos.xlsx\", engine=\"xlsxwriter\") as wr:\n",
        "    ct2_enriq.to_excel(wr, index=False, sheet_name=\"CT2_enriquecido\")\n",
        "    se2_enriq.to_excel(wr, index=False, sheet_name=\"SE2_enriquecido\")\n",
        "\n",
        "# ------- Exporta PDFs compactos -------\n",
        "write_pdf_table_wrapped(\n",
        "    str(out_base / \"CT2_enriquecido.pdf\"),\n",
        "    \"CT2 — enriquecido (chaves + NF/CodigoParte/Fornecedor)\",\n",
        "    ct2_enriq,\n",
        "    keep_columns=cols_ct2_enriq,\n",
        "    drop_hist=True,\n",
        "    font_size=7\n",
        ")\n",
        "\n",
        "write_pdf_table_wrapped(\n",
        "    str(out_base / \"SE2_enriquecido.pdf\"),\n",
        "    \"SE2 — enriquecido (chaves + NF/CodigoParte/Fornecedor)\",\n",
        "    se2_enriq,\n",
        "    keep_columns=[c for c in se2_enriq.columns],\n",
        "    drop_hist=True,\n",
        "    font_size=7\n",
        ")\n",
        "\n",
        "print(\"CT2 & SE2 enriquecidos exportados (datas normalizadas).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc03ef4e",
      "metadata": {
        "id": "cc03ef4e"
      },
      "source": [
        "## 7) Match intra-CT2 (R1/R2/R3) + export PDF/Excel de pares\n",
        "O que faz: tenta parear débitos e créditos dentro da própria CT2 (mesma conta) por camadas de regras:\n",
        "\n",
        "R1: K_NF|CodigoParte\n",
        "\n",
        "R2: K_CodigoParte|Valor (com tolerância: procura valores ± tolerância em centavos)\n",
        "\n",
        "R3: K_NF|Valor (com tolerância)\n",
        "\n",
        "Lógica: agrupa por Conta + chave + sinal (D/C) e vai emparelhando 1 a 1. Quando encontra par, remove os índices do “pool” aberto e continua.\n",
        "\n",
        "Saídas:\n",
        "\n",
        "ct2_pairs: tabela com idx_D, idx_C, Conta, Regra, Diff_cent (diferença entre valores em centavos).\n",
        "\n",
        "ct2_restantes: sobras da CT2 ainda em aberto (não pareadas internamente).\n",
        "\n",
        "Arquivos: CT2_pairs_matches.xlsx e CT2_pairs_matches.pdf (amostra das primeiras linhas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "17d05d6e",
      "metadata": {
        "id": "17d05d6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "tol=int(tolerancia_centavos)\n",
        "def match_in_layers(df):\n",
        "    df=df.copy(); df[\"sign\"]=np.where(df[\"ValorDC\"]>=0,1,-1)\n",
        "    open_idx=df.index.tolist(); pairs=[]\n",
        "    def try_match(key_cols, rule_name):\n",
        "        nonlocal open_idx,pairs\n",
        "        grp=df.loc[open_idx].groupby([\"Conta\"]+key_cols+[\"sign\"])\n",
        "        from collections import defaultdict\n",
        "        buckets=defaultdict(lambda:{1:[], -1:[]})\n",
        "        for ix,_ in grp:\n",
        "            sign=ix[-1]; key=tuple(ix[1:-1])\n",
        "            for i in grp.indices[ix]: buckets[key][sign].append(i)\n",
        "        new=[]\n",
        "        for key,d in buckets.items():\n",
        "            pos,neg=d[1],d[-1]\n",
        "            while pos and neg:\n",
        "                i=pos.pop(); j=neg.pop()\n",
        "                vi=df.at[i,\"ValorAbsCent\"]; vj=df.at[j,\"ValorAbsCent\"]\n",
        "                diff=None if (pd.isna(vi) or pd.isna(vj)) else abs(int(vi)-int(vj))\n",
        "                if diff is None or diff<=tol: new.append((i,j,rule_name,diff))\n",
        "        flat=set([i for x in new for i in x[:2]])\n",
        "        open_idx=[i for i in open_idx if i not in flat]; pairs.extend(new)\n",
        "    try_match([\"K_NF|CodigoParte\"],\"R1\"); try_match([\"K_CodigoParte|Valor\"],\"R2\"); try_match([\"K_NF|Valor\"],\"R3\")\n",
        "    paired_rows=[]\n",
        "    for i,j,rule,diff in pairs:\n",
        "        d_idx=i if df.at[i,\"DC\"]==\"D\" else j; c_idx=j if df.at[i,\"DC\"]==\"D\" else i\n",
        "        paired_rows.append({\"idx_D\":d_idx,\"idx_C\":c_idx,\"Conta\":df.at[i,\"Conta\"],\"Regra\":rule,\"Diff_cent\":diff})\n",
        "    return pd.DataFrame(paired_rows), df.loc[open_idx].copy()\n",
        "ct2_pairs, ct2_restantes = match_in_layers(visao_dc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17dc4204",
      "metadata": {
        "id": "17dc4204"
      },
      "source": [
        "## 8) CT2 abertos x correspondência SE2 (export dos COM correspondência)\n",
        "O que faz: pega os abertos (restantes) da CT2 nas contas foco (Clientes/Fornecedores) e marca se existe correspondência na SE2 pelas mesmas regras (R1/R2/R3).\n",
        "\n",
        "Tolerância: para as chaves que incluem Valor, procura variações ± tolerancia_centavos.\n",
        "\n",
        "Saídas:\n",
        "\n",
        "abertos_ct2_conf: CT2 abertos com flags R1, R2, R3 e SE2_Presente (True se qualquer regra bater).\n",
        "\n",
        "abertos_com_se2: somente os que têm correspondência (para auditoria do que “bateu”).\n",
        "\n",
        "Arquivos: CT2_abertos_com_correspondencia_SE2.xlsx e CT2_abertos_com_correspondencia_SE2.pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e7d6f0f8",
      "metadata": {
        "id": "e7d6f0f8"
      },
      "outputs": [],
      "source": [
        "\n",
        "contas_foco={conta_clientes, conta_fornecedores}\n",
        "abertos_ct2=ct2_restantes[ct2_restantes[\"Conta\"].isin(contas_foco)].copy()\n",
        "def lookup_match_flags(df_ct2, df_se2):\n",
        "    keys=[(\"K_NF|CodigoParte\",\"R1\"),(\"K_CodigoParte|Valor\",\"R2\"),(\"K_NF|Valor\",\"R3\")]\n",
        "    tol=int(tolerancia_centavos); flags={k:[] for _,k in keys}; any_flag=[]\n",
        "    for _, r in df_ct2.iterrows():\n",
        "        has_any=False\n",
        "        for key_col, rule in keys:\n",
        "            k=r.get(key_col,\"\"); found=False\n",
        "            if pd.notna(k) and k!=\"\":\n",
        "                if \"Valor\" in key_col:\n",
        "                    base=r.get(\"ValorAbsCent\")\n",
        "                    if pd.isna(base): mask=(df_se2[key_col]==k)\n",
        "                    else:\n",
        "                        candidates=[str(int(base+d)) for d in range(-tol,tol+1)]\n",
        "                        left=str(k).split(\"|\")[0]; keys_cand=[f\"{left}|{c}\" for c in candidates]\n",
        "                        mask=df_se2[key_col].isin(keys_cand)\n",
        "                else: mask=(df_se2[key_col]==k)\n",
        "                found=bool(mask.any())\n",
        "            flags[rule].append(found); has_any=has_any or found\n",
        "        any_flag.append(has_any)\n",
        "    df_flags=pd.DataFrame(flags); df_flags[\"SE2_Presente\"]=any_flag; return df_flags\n",
        "abertos_flags=lookup_match_flags(abertos_ct2, se2)\n",
        "abertos_ct2_conf=pd.concat([abertos_ct2.reset_index(drop=True), abertos_flags], axis=1)\n",
        "abertos_com_se2=abertos_ct2_conf[abertos_ct2_conf[\"SE2_Presente\"]].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41671091",
      "metadata": {
        "id": "41671091"
      },
      "source": [
        "## 9) Saldo CT2 — Abertos (Excel + PDF)\n",
        "O que faz: relatório simples dos abertos (após match) com coluna SE2 (True/False).\n",
        "\n",
        "Colunas: Data, Lote, Hist, NF, CodigoParte, ValorDC, SE2.\n",
        "\n",
        "Arquivos: Saldo_CT2_abertos.xlsx e Saldo_CT2_abertos.pdf (até 200 linhas no PDF para visual rápido)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c51f2245",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c51f2245",
        "outputId": "e1c2529d-21bb-485d-9e2b-bb0090aeb8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saldo_CT2_abertos.xlsx salvo com aba de RESUMO e valores formatados (2 casas decimais).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 9) Saldo CT2 — Abertos (layout + TOTAL por conta dentro do mesmo arquivo, com 2 casas decimais)\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'abertos_ct2_conf' in globals(), \"Execute os itens anteriores.\"\n",
        "\n",
        "# ---- base e layout exigido ----\n",
        "abertos_ct2_simple = abertos_ct2_conf.copy()\n",
        "abertos_ct2_simple[\"SE2\"] = abertos_ct2_simple[\"SE2_Presente\"].fillna(False)\n",
        "\n",
        "df_out = (\n",
        "    abertos_ct2_simple.rename(columns={\n",
        "        \"Conta\":\"conta\",\n",
        "        \"CodigoParte\":\"CodParte\",\n",
        "        \"ValorDC\":\"VlrDC\"\n",
        "    })[[\"Data\",\"conta\",\"Lote\",\"Hist\",\"NF\",\"CodParte\",\"VlrDC\",\"SE2\"]]\n",
        "     .sort_values([\"conta\",\"Data\"], kind=\"stable\")\n",
        "     .copy()\n",
        ")\n",
        "\n",
        "# ---- totais por conta (para anexar no final do relatório) ----\n",
        "totais = (\n",
        "    df_out.groupby(\"conta\", dropna=False)[\"VlrDC\"]\n",
        "          .sum()\n",
        "          .reset_index()\n",
        "          .rename(columns={\"VlrDC\":\"Total_VlrDC\"})\n",
        "          .sort_values(\"conta\", kind=\"stable\")\n",
        ")\n",
        "\n",
        "total_rows = (\n",
        "    totais.assign(Data=\"\", Lote=\"\", Hist=\"\", NF=\"\", CodParte=\"\", SE2=\"\")\n",
        "          [[\"Data\",\"conta\",\"Lote\",\"Hist\",\"NF\",\"CodParte\",\"Total_VlrDC\",\"SE2\"]]\n",
        "          .rename(columns={\"Total_VlrDC\":\"VlrDC\"})\n",
        ")\n",
        "\n",
        "df_out_com_totais = pd.concat([df_out, total_rows], ignore_index=True)\n",
        "\n",
        "# ---- gravação em Excel COM formatação numérica (duas casas + milhar) ----\n",
        "out_path = Path(outdir) / \"Saldo_CT2_abertos.xlsx\"\n",
        "with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as wr:\n",
        "    # aba principal\n",
        "    df_out_com_totais.to_excel(wr, index=False, sheet_name=\"Saldo_CT2_abertos\")\n",
        "    ws1 = wr.sheets[\"Saldo_CT2_abertos\"]\n",
        "\n",
        "    # aba de resumo dentro do MESMO arquivo (nome solicitado)\n",
        "    resumo_nome = \"Saldo_CT2_abertos_resumo\"\n",
        "    totais.to_excel(wr, index=False, sheet_name=resumo_nome)\n",
        "    ws2 = wr.sheets[resumo_nome]\n",
        "\n",
        "    # formato com 2 casas decimais; Excel adaptará separadores ao locale (ex.: 1.000,00 em PT-BR)\n",
        "    wb = wr.book\n",
        "    fmt_2c = wb.add_format({\"num_format\": \"#,##0.00\"})\n",
        "\n",
        "    # aplica ao VlrDC da aba principal (coluna G = índice 6) e à Total_VlrDC da aba de resumo (coluna B = índice 1)\n",
        "    # (se mudar ordem das colunas no futuro, calculamos o índice dinamicamente)\n",
        "    def _col_letter(i):  # 0->A, 1->B...\n",
        "        letters = \"\"\n",
        "        i += 1\n",
        "        while i:\n",
        "            i, r = divmod(i-1, 26)\n",
        "            letters = chr(65+r) + letters\n",
        "        return letters\n",
        "\n",
        "    col_vlrdc_idx = df_out_com_totais.columns.get_loc(\"VlrDC\")\n",
        "    col_total_idx = totais.columns.get_loc(\"Total_VlrDC\")\n",
        "\n",
        "    ws1.set_column(f\"{_col_letter(col_vlrdc_idx)}:{_col_letter(col_vlrdc_idx)}\", 14, fmt_2c)\n",
        "    ws2.set_column(f\"{_col_letter(col_total_idx)}:{_col_letter(col_total_idx)}\", 14, fmt_2c)\n",
        "\n",
        "# ---- PDF principal (com totais ao final) ----\n",
        "write_pdf_table_wrapped(\n",
        "    str(Path(outdir) / \"Saldo_CT2_abertos.pdf\"),\n",
        "    \"Saldo CT2 — Abertos (com total por conta ao final)\",\n",
        "    df_out_com_totais,\n",
        "    keep_columns=[\"Data\",\"conta\",\"Lote\",\"Hist\",\"NF\",\"CodParte\",\"VlrDC\",\"SE2\"],\n",
        "    drop_hist=False,\n",
        "    font_size=7\n",
        ")\n",
        "\n",
        "print(\"Saldo_CT2_abertos.xlsx salvo com aba de RESUMO e valores formatados (2 casas decimais).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbfb6a09",
      "metadata": {
        "id": "cbfb6a09"
      },
      "source": [
        "## 10) CT2 PARES — novo formato (linhas + resumo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "94ff997b",
      "metadata": {
        "id": "94ff997b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 10) CT2 PARES — layout solicitado (Data, conta, NF, CodParte, valor debito, valor credito, debito - credito = 0)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'ct2_pairs' in globals() and 'visao_dc' in globals(), \"Execute os itens anteriores.\"\n",
        "\n",
        "def _first_non_empty(a, b):\n",
        "    a = \"\" if pd.isna(a) else str(a)\n",
        "    b = \"\" if pd.isna(b) else str(b)\n",
        "    return a if a.strip() != \"\" else b\n",
        "\n",
        "rows = []\n",
        "for _, par in ct2_pairs.iterrows():\n",
        "    d = visao_dc.loc[par[\"idx_D\"]]\n",
        "    c = visao_dc.loc[par[\"idx_C\"]]\n",
        "\n",
        "    # Data: menor entre Débito e Crédito (fallback para texto original se NaT)\n",
        "    d_dt = pd.to_datetime(d[\"Data\"], errors=\"coerce\")\n",
        "    c_dt = pd.to_datetime(c[\"Data\"], errors=\"coerce\")\n",
        "    if pd.notna(d_dt) or pd.notna(c_dt):\n",
        "        data = min([x for x in [d_dt, c_dt] if pd.notna(x)]).date().isoformat()\n",
        "    else:\n",
        "        data = str(d[\"Data\"])  # fallback\n",
        "\n",
        "    conta = _first_non_empty(d.get(\"Conta\"), c.get(\"Conta\"))\n",
        "    nf    = _first_non_empty(d.get(\"NF\"), c.get(\"NF\"))\n",
        "    cod   = _first_non_empty(d.get(\"CodigoParte\"), c.get(\"CodigoParte\"))\n",
        "\n",
        "    # Valores absolutos\n",
        "    val_deb  = abs(float(d[\"ValorDC\"])) if d[\"DC\"] == \"D\" else abs(float(c[\"ValorDC\"]))\n",
        "    val_cred = abs(float(c[\"ValorDC\"])) if c[\"DC\"] == \"C\" else abs(float(d[\"ValorDC\"]))\n",
        "\n",
        "    # Diferença (em moeda) e checagem por centavos dentro da tolerância\n",
        "    dif_moeda = round(val_deb - val_cred, 2)\n",
        "    dif_cent  = abs(int(round(val_deb*100)) - int(round(val_cred*100)))\n",
        "\n",
        "    # Se dentro da tolerância de centavos, força visual = 0.00\n",
        "    if dif_cent <= int(tolerancia_centavos):\n",
        "        dif_moeda = 0.00\n",
        "\n",
        "    rows.append({\n",
        "        \"Data\": data,\n",
        "        \"conta\": conta,\n",
        "        \"NF\": nf,\n",
        "        \"CodParte\": cod,\n",
        "        \"valor debito\": round(val_deb, 2),\n",
        "        \"valor credito\": round(val_cred, 2),\n",
        "        \"debito - credito\": round(dif_moeda, 2),\n",
        "    })\n",
        "\n",
        "pares_layout = pd.DataFrame(rows).sort_values([\"conta\", \"Data\"], kind=\"stable\")\n",
        "\n",
        "# --- Resumo por conta (tem que zerar)\n",
        "resumo_pares = (\n",
        "    pares_layout.groupby(\"conta\", dropna=False)[[\"valor debito\",\"valor credito\",\"debito - credito\"]]\n",
        "               .agg(qtd=(\"valor debito\",\"count\"),\n",
        "                    valor_debito=(\"valor debito\",\"sum\"),\n",
        "                    valor_credito=(\"valor credito\",\"sum\"),\n",
        "                    saldo=(\"debito - credito\",\"sum\"))\n",
        "               .reset_index()\n",
        "               .loc[:, [\"conta\",\"qtd\",\"valor_debito\",\"valor_credito\",\"saldo\"]]\n",
        "               .sort_values(\"conta\", kind=\"stable\")\n",
        ")\n",
        "\n",
        "# --- Exporta Excel (duas abas)\n",
        "with pd.ExcelWriter(Path(outdir) / \"CT2_pairs_matches.xlsx\", engine=\"xlsxwriter\") as wr:\n",
        "    pares_layout.to_excel(wr, index=False, sheet_name=\"CT2_pairs_matches\")\n",
        "    resumo_pares.to_excel(wr, index=False, sheet_name=\"Resumo\")\n",
        "\n",
        "# --- Exporta PDFs (linhas + resumo)\n",
        "write_pdf_table_wrapped(\n",
        "    str(Path(outdir) / \"CT2_pairs_matches.pdf\"),\n",
        "    \"CT2 PARES — Linhas (por conta)\",\n",
        "    pares_layout,\n",
        "    keep_columns=[\"Data\",\"conta\",\"NF\",\"CodParte\",\"valor debito\",\"valor credito\",\"debito - credito\"],\n",
        "    drop_hist=True,\n",
        "    font_size=7\n",
        ")\n",
        "\n",
        "write_pdf_table_wrapped(\n",
        "    str(Path(outdir) / \"CT2_pairs_matches_resumo.pdf\"),\n",
        "    \"CT2 PARES — Resumo por conta\",\n",
        "    resumo_pares,\n",
        "    keep_columns=[\"conta\",\"qtd\",\"valor_debito\",\"valor_credito\",\"saldo\"],\n",
        "    drop_hist=True,\n",
        "    font_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d5a903c",
      "metadata": {
        "id": "5d5a903c"
      },
      "source": [
        "## 11) Pendências Finais + README\n",
        " que faz: gera as pendências = CT2 abertos (contas foco) que não têm presença na SE2 (SE2_Presente == False).\n",
        "\n",
        "Campos extras:\n",
        "\n",
        "competencia: derivada de Data (AAAA-MM) para facilitar filtros.\n",
        "\n",
        "Obs: coluna vazia com validação de dados (lista suspensa no Excel) para classificar o motivo (ex.: “Falta provisão”, “Duplicidade”, etc.).\n",
        "\n",
        "Conta_Foco: rotula “Clientes” / “Fornecedores” / “Outras”.\n",
        "\n",
        "Arquivos:\n",
        "\n",
        "MVP_pendencias_finais_v2.xlsx com duas abas: Pendencias_Finais e Pendencias_<competencia>.\n",
        "\n",
        "Pendencias_Finais_<competencia>.pdf (se não houver registros na competência, cai para um PDF “fallback” com geral)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "583ff1f2",
      "metadata": {
        "id": "583ff1f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "pendencias = abertos_ct2_conf[~abertos_ct2_conf[\"SE2_Presente\"]].copy()\n",
        "pendencias[\"competencia\"] = pd.to_datetime(pendencias[\"Data\"], errors=\"coerce\").dt.strftime(\"%Y-%m\")\n",
        "pendencias[\"Obs\"] = \"\"\n",
        "pendencias[\"Conta_Foco\"] = np.where(pendencias[\"Conta\"]==conta_clientes,\"Clientes\",\n",
        "                             np.where(pendencias[\"Conta\"]==conta_fornecedores,\"Fornecedores\",\"Outras\"))\n",
        "with pd.ExcelWriter(Path(outdir)/\"MVP_pendencias_finais_v2.xlsx\", engine=\"xlsxwriter\") as writer:\n",
        "    pendencias.to_excel(writer, index=False, sheet_name=\"Pendencias_Finais\")\n",
        "    (pendencias[pendencias[\"competencia\"]==competencia]).to_excel(writer, index=False, sheet_name=f\"Pendencias_{competencia}\")\n",
        "df_pdf = pendencias[pendencias[\"competencia\"]==competencia]\n",
        "title = f\"Pendências Finais - {competencia}\" if len(df_pdf)>0 else \"Pendências Finais - Todas as competências\"\n",
        "src   = df_pdf if len(df_pdf)>0 else pendencias\n",
        "cols_pend = [\"Data\",\"NF\",\"CodigoParte\",\"Conta\",\"DC\",\"ValorDC\",\"R1\",\"R2\",\"R3\",\"SE2_Presente\",\"competencia\",\"Obs\",\"Conta_Foco\"]\n",
        "write_pdf_table_wrapped(str(Path(outdir)/f\"Pendencias_Finais_{competencia}.pdf\"), title, src,\n",
        "                        keep_columns=cols_pend, drop_hist=True, font_size=7)\n",
        "with open(Path(outdir).parent / \"README_EXECUCAO.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(f\"README v5 gerado em {datetime.now().isoformat()}\\nOUT: {outdir}\\nTol(centavos): {tolerancia_centavos}\\nComp: {competencia}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e10b96",
      "metadata": {
        "id": "26e10b96"
      },
      "source": [
        "## 12) Exports auxiliares de conferência\n",
        "O que faz: produz duas planilhas de apoio:\n",
        "\n",
        "CT2_conciliacao_enriquecido.xlsx\n",
        "\n",
        "Visao_DC (todas as linhas D/C enriquecidas)\n",
        "\n",
        "Resumo_Conta (conferência de totais por conta e DC)\n",
        "\n",
        "MVP_CT2_intra_matches_e_conferencia.xlsx\n",
        "\n",
        "CT2_Matches_tol (pares D×C achados)\n",
        "\n",
        "CT2_Abertos (o que sobrou da CT2)\n",
        "\n",
        "Conferencia_CT2_vs_SE2 (abertos com flags de presença na SE2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cb7dd786",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb7dd786",
        "outputId": "e5d0ed9c-d770-42c9-fb93-2150a7d465b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos de conferência gerados:\n",
            " - /content/OUT/CT2_conciliacao_enriquecido.xlsx\n",
            " - /content/OUT/MVP_CT2_intra_matches_e_conferencia.xlsx\n"
          ]
        }
      ],
      "source": [
        "# 11A) Exports auxiliares de conferência\n",
        "# O que faz: cria dois arquivos de apoio:\n",
        "#   1) CT2_conciliacao_enriquecido.xlsx  -> Visao_DC + Resumo_Conta\n",
        "#   2) MVP_CT2_intra_matches_e_conferencia.xlsx -> CT2_Matches_tol + CT2_Abertos + Conferencia_CT2_vs_SE2\n",
        "#\n",
        "# Requisitos: variáveis visao_dc, ct2_pairs, ct2_restantes, abertos_ct2_conf já criadas nos itens anteriores.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'visao_dc' in globals(), \"Execute a geração da visão D/C.\"\n",
        "assert 'ct2_pairs' in globals(), \"Execute o pareamento intra-CT2.\"\n",
        "assert 'ct2_restantes' in globals(), \"Execute o pareamento intra-CT2.\"\n",
        "assert 'abertos_ct2_conf' in globals(), \"Execute a confirmação CT2 x SE2 (flags).\"\n",
        "\n",
        "# -------- 1) CT2_conciliacao_enriquecido.xlsx --------\n",
        "# Visao_DC (todas as linhas D/C enriquecidas)\n",
        "visao_dc_export = visao_dc.copy()\n",
        "\n",
        "# Resumo_Conta (conferência por Conta e DC)\n",
        "resumo_conta = (\n",
        "    visao_dc_export\n",
        "      .groupby([\"Conta\",\"DC\"], dropna=False)[\"ValorDC\"]\n",
        "      .agg(qtd=\"count\", soma=\"sum\", soma_abs=lambda s: s.abs().sum())\n",
        "      .reset_index()\n",
        "      .sort_values([\"Conta\",\"DC\"], kind=\"stable\")\n",
        ")\n",
        "\n",
        "out1 = Path(outdir) / \"CT2_conciliacao_enriquecido.xlsx\"\n",
        "with pd.ExcelWriter(out1, engine=\"xlsxwriter\") as wr:\n",
        "    visao_dc_export.to_excel(wr, index=False, sheet_name=\"Visao_DC\")\n",
        "    resumo_conta.to_excel(wr, index=False, sheet_name=\"Resumo_Conta\")\n",
        "\n",
        "    # formata colunas numéricas com 2 casas no resumo\n",
        "    wb = wr.book\n",
        "    fmt2 = wb.add_format({\"num_format\": \"#,##0.00\"})\n",
        "    ws_r = wr.sheets[\"Resumo_Conta\"]\n",
        "    for col_name in [\"soma\",\"soma_abs\"]:\n",
        "        if col_name in resumo_conta.columns:\n",
        "            col_idx = resumo_conta.columns.get_loc(col_name)\n",
        "            # aplica a coluna inteira\n",
        "            ws_r.set_column(col_idx, col_idx, 14, fmt2)\n",
        "\n",
        "# -------- 2) MVP_CT2_intra_matches_e_conferencia.xlsx --------\n",
        "# CT2_Matches_tol (pares D×C achados)\n",
        "ct2_matches_tol = ct2_pairs.copy()  # mantém: idx_D, idx_C, Conta, Regra, Diff_cent\n",
        "\n",
        "# CT2_Abertos (o que sobrou da CT2 após pareamento)\n",
        "ct2_abertos = ct2_restantes.copy()\n",
        "\n",
        "# Conferencia_CT2_vs_SE2 (abertos com flags de presença na SE2)\n",
        "conferencia = abertos_ct2_conf.copy()\n",
        "\n",
        "out2 = Path(outdir) / \"MVP_CT2_intra_matches_e_conferencia.xlsx\"\n",
        "with pd.ExcelWriter(out2, engine=\"xlsxwriter\") as wr:\n",
        "    ct2_matches_tol.to_excel(wr, index=False, sheet_name=\"CT2_Matches_tol\")\n",
        "    ct2_abertos.to_excel(wr, index=False, sheet_name=\"CT2_Abertos\")\n",
        "    conferencia.to_excel(wr, index=False, sheet_name=\"Conferencia_CT2_vs_SE2\")\n",
        "\n",
        "print(\"Arquivos de conferência gerados:\")\n",
        "print(\" -\", out1)\n",
        "print(\" -\", out2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e471af",
      "metadata": {
        "id": "95e471af"
      },
      "source": [
        "## 13) ML (supervisionado + anomalias)\n",
        "Objetivo: auxiliar priorização/triagem do que provavelmente tem (ou não tem) correspondência na SE2.\n",
        "\n",
        "Rótulo: SE2_Presente (1/0) gerado na etapa 8.\n",
        "\n",
        "Features:\n",
        "\n",
        "Numéricas: valor_abs, nf_len, cod_len, hist_len, ano, mes, dia.\n",
        "\n",
        "Categóricas: conta_cat, dc_cat (OneHotEncoder aplicado em pipeline).\n",
        "\n",
        "Sinais auxiliares: is_debito, is_credito.\n",
        "\n",
        "Modelo: RandomForestClassifier(class_weight=\"balanced\").\n",
        "\n",
        "Treina/testa (split 75/25) somente se houver variedade suficiente de rótulos e amostra mínima.\n",
        "\n",
        "Calcula métricas: Accuracy, F1, ROC_AUC + Matriz de Confusão.\n",
        "\n",
        "Anomalias (IsolationForest):\n",
        "\n",
        "Rodado apenas sobre os casos onde SE2_Presente == 0 (não bateu) — para destacar outliers.\n",
        "\n",
        "Scores e prioridade:\n",
        "\n",
        "rf_score (probabilidade do modelo), iso_score (quando disponível),\n",
        "\n",
        "Prioridade: Alta/Média/Baixa (regras simples combinando RF e anomalia).\n",
        "\n",
        "Saídas:\n",
        "\n",
        "ML_predictions.xlsx com abas:\n",
        "\n",
        "Predicoes_Todas (dataset com scores),\n",
        "\n",
        "Top_Suspeitos (não-batidos priorizados),\n",
        "\n",
        "Matriz_Confusao, Metricas.\n",
        "\n",
        "Artefatos: ML_ct2_se2_classifier.pkl, ML_ct2_isolation_forest.pkl (se treinado), ML_feature_columns.json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f8f465cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8f465cb",
        "outputId": "a4b4efc6-ee3a-447c-e000-fe68edf527a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF (hold-out) -> ACC: 0.7929 | F1: 0.7752 | ROC_AUC: 0.8901\n",
            "OK: ML_predictions.xlsx (CV + predições + suspeitos + métricas) e modelos .pkl salvos em OUT.\n"
          ]
        }
      ],
      "source": [
        "# 13) ML (supervisionado + anomalias) — BLOCO COMPLETO E ENXUTO\n",
        "\n",
        "# O que faz:\n",
        "#  - 13.0) Pré-ML: fixa seed, monta dataset de features, roda baseline/comparação (CV) entre Dummy, LogReg e RandomForest.\n",
        "#  - 13.1) Treino hold-out com RandomForest e detecção de anomalias (IsolationForest) nos \"ausentes\".\n",
        "#  - 13.2) Geração de outputs: ML_predictions.xlsx (abas Comparacao_CV, Predicoes_Todas, Top_Suspeitos, Matriz_Confusao, Metricas)\n",
        "#           + modelos .pkl e lista de features em OUT.\n",
        "# Requisitos:\n",
        "#  - Variável `abertos_ct2_conf` (do item CT2 × SE2) já carregada\n",
        "#  - Variável `outdir` definida (se não, cai no fallback \"./OUT\")\n",
        "\n",
        "import os, random, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from joblib import dump\n",
        "\n",
        "# ---------- 13.0) Pré-ML: seed + dataset + baseline/comparação (CV) ----------\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "assert 'abertos_ct2_conf' in globals(), \"Execute o item CT2×SE2 antes do ML (precisa de 'abertos_ct2_conf').\"\n",
        "\n",
        "# Função util local (se não veio de itens anteriores)\n",
        "def _as_str(series):\n",
        "    return series.astype(\"string\").fillna(\"\")\n",
        "\n",
        "# Monta dataset de ML (se já existir, reaproveita)\n",
        "if 'data_ml' not in globals():\n",
        "    data_ml = abertos_ct2_conf.copy()\n",
        "    data_ml[\"is_debito\"]  = (data_ml[\"DC\"]==\"D\").astype(int)\n",
        "    data_ml[\"is_credito\"] = (data_ml[\"DC\"]==\"C\").astype(int)\n",
        "    data_ml[\"valor_abs\"]  = data_ml[\"ValorDC\"].abs().fillna(0.0)\n",
        "    data_ml[\"nf_len\"]     = _as_str(data_ml[\"NF\"]).apply(len)\n",
        "    data_ml[\"cod_len\"]    = _as_str(data_ml[\"CodigoParte\"]).apply(len)\n",
        "    data_ml[\"hist_len\"]   = _as_str(data_ml[\"Hist\"]).apply(len)\n",
        "    data_ml[\"ano\"]        = pd.to_datetime(data_ml[\"Data\"], errors=\"coerce\").dt.year.fillna(0).astype(int)\n",
        "    data_ml[\"mes\"]        = pd.to_datetime(data_ml[\"Data\"], errors=\"coerce\").dt.month.fillna(0).astype(int)\n",
        "    data_ml[\"dia\"]        = pd.to_datetime(data_ml[\"Data\"], errors=\"coerce\").dt.day.fillna(0).astype(int)\n",
        "    data_ml[\"conta_cat\"]  = _as_str(data_ml[\"Conta\"])\n",
        "    data_ml[\"dc_cat\"]     = _as_str(data_ml[\"DC\"])\n",
        "\n",
        "if 'features' not in globals():\n",
        "    features = [\"valor_abs\",\"is_debito\",\"is_credito\",\"nf_len\",\"cod_len\",\"hist_len\",\n",
        "                \"ano\",\"mes\",\"dia\",\"conta_cat\",\"dc_cat\"]\n",
        "\n",
        "target   = \"SE2_Presente\"\n",
        "cat_cols = [\"conta_cat\",\"dc_cat\"]\n",
        "\n",
        "X = data_ml[features].copy()\n",
        "y = data_ml[target].astype(int)\n",
        "\n",
        "pre = ColumnTransformer([(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)],\n",
        "                        remainder=\"passthrough\")\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Baseline e comparação via CV\n",
        "modelos = {\n",
        "    \"Dummy(stratified)\": DummyClassifier(strategy=\"stratified\", random_state=SEED),\n",
        "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=SEED)\n",
        "}\n",
        "\n",
        "resultados = []\n",
        "if y.nunique() >= 2 and len(X) >= 10:\n",
        "    for nome, est in modelos.items():\n",
        "        pipe = Pipeline([(\"pre\", pre), (\"est\", est)])\n",
        "        f1  = cross_val_score(pipe, X, y, cv=cv, scoring=\"f1\").mean()\n",
        "        acc = cross_val_score(pipe, X, y, cv=cv, scoring=\"accuracy\").mean()\n",
        "        resultados.append({\"modelo\": nome, \"F1_cv\": round(f1,4), \"ACC_cv\": round(acc,4)})\n",
        "else:\n",
        "    resultados.append({\"modelo\": \"Dados insuficientes\", \"F1_cv\": np.nan, \"ACC_cv\": np.nan})\n",
        "\n",
        "df_resultados = pd.DataFrame(resultados).sort_values(\"F1_cv\", ascending=False)\n",
        "\n",
        "# ---------- 13.1) Treino hold-out (RF) + anomalias (IsolationForest) ----------\n",
        "clf = Pipeline([(\"pre\", pre),\n",
        "                (\"rf\", RandomForestClassifier(class_weight=\"balanced\", n_estimators=200, random_state=SEED))])\n",
        "\n",
        "trained = False\n",
        "acc=f1=auc=np.nan\n",
        "y_test=y_pred=None\n",
        "\n",
        "if y.nunique()>1 and len(X)>20:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=SEED, stratify=y\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)[:,1]\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1  = f1_score(y_test, y_pred)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, y_prob)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    print(\"RF (hold-out) -> ACC:\", round(acc,4), \"| F1:\", round(f1,4), \"| ROC_AUC:\", round(auc if pd.notna(auc) else 0,4))\n",
        "    trained = True\n",
        "else:\n",
        "    print(\"ML: dados insuficientes para hold-out (rótulo único ou amostra pequena).\")\n",
        "\n",
        "# ---------- 13.2) Outputs: Excel + modelos ----------\n",
        "if 'outdir' not in globals() or not outdir:\n",
        "    outdir = \"./OUT\"\n",
        "Path(outdir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if trained:\n",
        "    # IsolationForest apenas nos ausentes (negativos)\n",
        "    data_ml[\"iso_score\"] = np.nan\n",
        "    X_no = data_ml.loc[data_ml[target]==0, features].copy()\n",
        "    if len(X_no) >= 10:\n",
        "        iso_pre = ColumnTransformer([(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)],\n",
        "                                    remainder=\"passthrough\")\n",
        "        X_no_enc = iso_pre.fit_transform(X_no)\n",
        "        iso = IsolationForest(random_state=SEED, contamination=0.10)\n",
        "        iso.fit(X_no_enc)\n",
        "        iso_scores = -iso.decision_function(X_no_enc)\n",
        "        data_ml.loc[data_ml[target]==0, \"iso_score\"] = iso_scores\n",
        "\n",
        "    # Probabilidades para todas as linhas\n",
        "    data_ml[\"rf_score\"] = clf.predict_proba(X)[:,1]\n",
        "\n",
        "    # Priorização\n",
        "    def prior(row):\n",
        "        s = row.get(\"rf_score\", 0.0)\n",
        "        if row[target]==1:  # já confirmado na SE2 -> baixa prioridade\n",
        "            return \"Baixa\"\n",
        "        if pd.notna(row.get(\"iso_score\")) and row[\"iso_score\"] > np.nanpercentile(data_ml[\"iso_score\"], 85):\n",
        "            return \"Alta\"\n",
        "        if s>=0.66: return \"Alta\"\n",
        "        if s>=0.33: return \"Média\"\n",
        "        return \"Baixa\"\n",
        "    data_ml[\"Prioridade\"] = data_ml.apply(prior, axis=1)\n",
        "\n",
        "    # Salva modelos e metadados\n",
        "    dump(clf, str(Path(outdir)/\"ML_ct2_se2_classifier.pkl\"))\n",
        "    if 'iso' in locals():\n",
        "        dump(iso, str(Path(outdir)/\"ML_ct2_isolation_forest.pkl\"))\n",
        "    with open(Path(outdir)/\"ML_feature_columns.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "        json.dump(features, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Excel consolidado\n",
        "    pred_all = data_ml.copy().sort_values([\"Prioridade\",\"rf_score\"], ascending=[True, False])\n",
        "    top_suspeitos = pred_all[pred_all[target]==0].head(200)\n",
        "\n",
        "    with pd.ExcelWriter(str(Path(outdir)/\"ML_predictions.xlsx\"), engine=\"xlsxwriter\") as wr:\n",
        "        # Comparação CV\n",
        "        df_resultados.to_excel(wr, index=False, sheet_name=\"Comparacao_CV\")\n",
        "        # Predições\n",
        "        pred_all.to_excel(wr, index=False, sheet_name=\"Predicoes_Todas\")\n",
        "        top_suspeitos.to_excel(wr, index=False, sheet_name=\"Top_Suspeitos\")\n",
        "        # Métricas hold-out (se houver)\n",
        "        if y_test is not None and y_pred is not None:\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            cm_df = pd.DataFrame(cm, index=[\"Neg\",\"Pos\"], columns=[\"Pred_Neg\",\"Pred_Pos\"])\n",
        "            cm_df.to_excel(wr, sheet_name=\"Matriz_Confusao\")\n",
        "            pd.DataFrame([{\"Accuracy\":acc,\"F1\":f1,\"ROC_AUC\":auc}]).to_excel(wr, index=False, sheet_name=\"Metricas\")\n",
        "\n",
        "    print(\"OK: ML_predictions.xlsx (CV + predições + suspeitos + métricas) e modelos .pkl salvos em OUT.\")\n",
        "else:\n",
        "    # Mesmo sem treino, registre a comparação CV\n",
        "    with pd.ExcelWriter(str(Path(outdir)/\"ML_predictions.xlsx\"), engine=\"xlsxwriter\") as wr:\n",
        "        df_resultados.to_excel(wr, index=False, sheet_name=\"Comparacao_CV\")\n",
        "    print(\"Aviso: ML sem hold-out (dados insuficientes). Exportado apenas Comparacao_CV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) Gerar arquivo ZIP com as análises realizadas.\n"
      ],
      "metadata": {
        "id": "8-SxrdMlchxZ"
      },
      "id": "8-SxrdMlchxZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Gera um ZIP com tudo que está em /content/OUT e baixa\n",
        "import shutil\n",
        "from google.colab import files\n",
        "zip_path = \"/content/relatorios_out\"\n",
        "shutil.make_archive(zip_path, \"zip\", \"/content/OUT\")\n",
        "files.download(zip_path + \".zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WrBAdaoSa5DZ",
        "outputId": "00b91fd8-34f5-4bf0-90c1-01ae2934b8e7"
      },
      "id": "WrBAdaoSa5DZ",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_489ce5a4-d77b-4a6a-a637-41cf134fce6a\", \"relatorios_out.zip\", 1645942)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}